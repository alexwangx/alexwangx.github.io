<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Apache Spark :: The Cuddly Computing Machine</title>
    <link rel="canonical" href="http://alexwangx.github.io/cuddly-blog/1.2/spark/spark-internals/root/spark-overview.html">
    <meta name="generator" content="Antora 2.3.1">
    <link rel="stylesheet" href="../../../../../_/css/site.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-86782445-1"></script>
    <script>function gtag(){dataLayer.push(arguments)};window.dataLayer=window.dataLayer||[];gtag('js',new Date());gtag('config','UA-86782445-1')</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="http://alexwangx.github.io">The Cuddly Computing Machine</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/alexwangx">GitHub</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="cuddly-blog" data-version="1.2">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../../../index.html">Cuddly Blog</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Flink</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../../../flink/training/index.html">Flink Training Home</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Streaming 介绍</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/intro/intro-1.html">使用Apache Flink 进行流处理</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/intro/intro-2.html">流可以传输什么?</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/intro/intro-3.html">一个完整的例子</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">环境配置</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/labs/devEnvSetup.html">设置您的开发环境</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/labs/taxiData.html">使用出租车数据流</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/labs/howto-exercises.html">如何做实验室</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/training/lab1/rideCleansing.html">Lab 1 - 过滤流</a>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">数据转换</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab1/stateless.html">无状态转换</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab1/keyed-streams.html">Keyed 流</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab1/stateful.html">有状态的转换</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab1/connected-streams.html">连接流</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/training/lab2/rideEnrichment-flatmap.html">Lab 2 - Stateful Enrichment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">时间和分析</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab3/event-time-watermarks.html">Event Time and Watermarks</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab3/windows.html">Windows</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Lab 3 - Windowing</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab3/hourlyTips.html">Lab 3 - 窗口化分析</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab3/hourlyTips-discussion.html">Lab 3 - 讨论</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">事件驱动的应用</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab4/processfunction.html">ProcessFunction</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab4/side-outputs.html">Side Outputs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Lab 4 - 到期状态</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab4/rideEnrichment-processfunction.html">Lab 4 - 练习</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/lab4/expiring-state-discussion.html">Lab 4 - 讨论</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">容错能力</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/fault/state-backends.html">状态后端</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/fault/snapshots.html">检查点和保存点</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/training/app-dev.html">应用开发</a>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">其他练习</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/extra-labs/longRides.html">长途警报</a>
  </li>
  <li class="nav-item" data-depth="4">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">广播状态</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="5">
    <a class="nav-link" href="../../../flink/training/extra-labs/nearestTaxi.html">寻找最近的出租车</a>
  </li>
  <li class="nav-item" data-depth="5">
    <a class="nav-link" href="../../../flink/training/extra-labs/ongoingRides.html">报告正在进行的出租车</a>
  </li>
  <li class="nav-item" data-depth="5">
    <a class="nav-link" href="../../../flink/training/extra-labs/taxiQuery.html">规则引擎</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../flink/training/extra-labs/eventTimeJoin.html">丰富的 Joins</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/training/resources.html">资源链接</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../../../flink/introduction/index.html">Flink 简介</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/architecture.html">Flink 架构介绍</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/failover.html">Flink 容错机制</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/scheduling.html">Flink 调度机制</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/statemachine.html">Flink 状态机</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/memorymanager.html">Flink 内存管理</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/dataflow.html">Flink Dataflow</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/batchdata.html">Flink 批处理</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../flink/introduction/streamdata.html">Flink 流处理</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../flink/internals/index.html">Flink 学习笔记</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Spark</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Spark ops</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../ops/spark-exitcode.html">Spark 任务退出码</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../ops/spark-issues.html">Spark 常见错误</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="https://alexwangx.github.io/apache-spark-internals/2.4.5/">Spark 2.4.4 架构原理</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Hadoop</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../hadoop/hadoop_krb_install.html">Hadoop 配置 Kerberos</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Hadoop ops</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../hadoop/hdp-ops/hadoop_commands.html">Hadoop Commands</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../hadoop/hdp-ops/hadoop_practices.html">Hadoop Practices</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../hadoop/hdp-ops/hadoop_jmx.html">Hadoop JMX</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../hadoop/hdp-ops/hadoop_delroot.html">Hadoop 意外删除挽救</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../hadoop/hdp-ops/delrmstore_onzk.html">删除ZK上的rmstore目录</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Hbase</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Hbase ops</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../hbase/hbase-ops/hbase_notes.html">Hbase 基本命令</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../hbase/hbase-ops/hbase_issues.html">Hbase Issues</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Druid</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../druid/druid-install.html">Druid 安装</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../druid/ops/druid-restApi.html">Druid RestApi</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Druid 架构</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../../../druid/druid-internals/tutorial-load-data.html">Druid 导入数据</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../../../druid/druid-internals/tutorial-load-data1.html">Druid Json 配置介绍</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/druid-internals.html">Druid 0.1.5 架构</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/broker.html">Druid Broker</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/coordinator.html">Druid Coordinator</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/historical.html">Druid Historical</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/middleManager.html">Druid MiddleManager</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/overlord.html">Druid Overlord</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/router.html">Druid Router</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../../druid/druid-internals/segment.html">Druid Segments</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Kafka</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../kafka/kafka_practices.html">Kafka Practices</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../kafka/kafka_issues.html">Kafka Issues</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Other</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../mac.html">Mac OS</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../linux.html">Linux</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../mysql_install.html">Mysql 安装</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../readlist.html">北京中小学孩子读书清单</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../../demo/demo.html">Adoc Demo</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Cuddly Blog</span>
    <span class="version">1.2</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">Cuddly Blog</span>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../../../index.html">1.2</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main>
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../../../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../../../index.html">Cuddly Blog</a></li>
    <li><a href="spark-overview.html">Apache Spark</a></li>
  </ul>
</nav>
</div>
<article class="doc">
<h1 class="page">Apache Spark</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><a href="http://spark.apache.org/">Apache Spark</a> 是一个 <strong>开源分布式通用集群计算框架</strong> 主要是 <strong>基于内存的数据处理引擎</strong>
应用到大规模数据处理场景: 如 ETL, 数据分析,机器学习和图像处理等.
在批处理及流处理方面为下列程序语言: Scala, Python, Java, R, 和 SQL, 提供了<a href="#unified-api">丰富, 简洁的高级API</a></p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="../../_images/spark-internals/diagrams/spark-platform.png" alt="spark platform">
</div>
<div class="title">Figure 1. The Spark Platform</div>
</div>
<div class="paragraph">
<p>您也可以将 Spark 用于 <strong>批处理和流处理模式</strong> 的分布式数据处理引擎, 具有SQL查询, 图处理和机器学习功能.</p>
</div>
<div class="paragraph">
<p>与Hadoop的基于磁盘的两阶段MapReduce计算引擎相比, Spark的多阶段 (主要是) 内存计算引擎可让您在内存中运行更多的计算.
因此, 大多数时候可以为某些应用提供更好的性能, 例如: 迭代算法或交互式数据挖掘
(<a href="https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html">Spark
officially sets a new record in large-scale sorting</a>).</p>
</div>
<div class="paragraph">
<p>Spark的目标是速度, 易用性, 可扩展性和交互式分析.</p>
</div>
<div class="paragraph">
<p>Spark通常被称为 <strong>计算引擎</strong> 或简称为 <strong>执行引擎</strong>.</p>
</div>
<div class="paragraph">
<p>Spark 是一个 <strong>用于执行复杂的多阶段应用程序的分布式平台</strong> , 例如 <strong>机器学习算法</strong> 和 <strong>交互式查询</strong> .
Spark 为内存集群计算提供了一种有效的抽象,
称为 <a href="spark-rdd.adoc">Resilient Distributed Dataset</a>.</p>
</div>
<div class="paragraph">
<p>使用Spark应用程序框架, Spark 简化了大规模机器学习和预测分析的使用.</p>
</div>
<div class="paragraph">
<p>Spark 主要用 <a href="http://scala-lang.org/">Scala</a> 编写, 提供了 Java, Python 和 R语言的开发API.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
微软的 <a href="https://github.com/Microsoft/Mobius">Mobius项目</a> 为Spark提供了 C＃ API
<em>"以 .NET 框架支持的语言 (如C＃ 或 F＃) 启用 Spark 驱动程序和数据处理操作的实现. "</em>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>如果有大量数据需要处理, 并且 MapReduce 程序无法提供低延迟处理, 那么Spark是一个可行的选择.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>支持多数据源, 多数据类型.</p>
</li>
<li>
<p>满足大量数据处理等需求.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Apache Spark project is an umbrella for <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/">SQL</a> (with Datasets), <a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/">streaming</a>, <a href="http://spark.apache.org/mllib/">machine learning</a> (pipelines) and <a href="http://spark.apache.org/graphx/">graph</a> processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API.</p>
</div>
<div class="paragraph">
<p>Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix).</p>
</div>
<div class="paragraph">
<p>Apache Spark&#8217;s <a href="https://jaceklaskowski.gitbooks.io/spark-structured-streaming/">Structured Streaming</a> and <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/">SQL</a> programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics.</p>
</div>
<div class="paragraph">
<p>At a high level, any Spark application creates <strong>RDDs</strong> out of some input, run <a href="spark-rdd.adoc">(lazy) transformations</a> of these RDDs to some other form (shape), and finally perform <a href="spark-rdd.adoc">actions</a> to collect or store data. Not much, huh?</p>
</div>
<div class="paragraph">
<p>You can look at Spark from programmer&#8217;s, data engineer&#8217;s and administrator&#8217;s point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to.</p>
</div>
<div class="paragraph">
<p>It is Spark&#8217;s goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
When you hear "Apache Spark" it can be two things&#8201;&#8212;&#8201;the Spark engine aka <strong>Spark Core</strong> or the Apache Spark open source project which is an "umbrella" term for Spark Core and the accompanying Spark Application Frameworks, i.e. <a href="spark-sql.adoc">Spark SQL</a>, <a href="spark-streaming/spark-streaming.adoc">Spark Streaming</a>, <a href="spark-mllib/spark-mllib.adoc">Spark MLlib</a> and <a href="spark-graphx.adoc">Spark GraphX</a> that sit on top of Spark Core and the main data abstraction in Spark called <a href="spark-rdd.adoc">RDD - Resilient Distributed Dataset</a>.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_spark"><a class="anchor" href="#_why_spark"></a><a id="why-spark"></a> Why Spark</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand.</p>
</div>
<div class="sect2">
<h3 id="_easy_to_get_started"><a class="anchor" href="#_easy_to_get_started"></a>Easy to Get Started</h3>
<div class="paragraph">
<p>Spark offers <a href="spark-shell.adoc">spark-shell</a> that makes for a very easy head start to writing and running Spark applications on the command line on your laptop.</p>
</div>
<div class="paragraph">
<p>You could then use <a href="spark-standalone.adoc">Spark Standalone</a> built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset.</p>
</div>
</div>
<div class="sect2">
<h3 id="_unified_engine_for_diverse_workloads"><a class="anchor" href="#_unified_engine_for_diverse_workloads"></a>Unified Engine for Diverse Workloads</h3>
<div class="paragraph">
<p>As said by Matei Zaharia - the author of Apache Spark - in <a href="https://youtu.be/49Hr5xZyTEA">Introduction to AmpLab Spark Internals video</a> (quoting with few changes):</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>One of the Spark project goals was to deliver a platform that supports a very wide array of <strong>diverse workflows</strong> - not only MapReduce <strong>batch</strong> jobs (there were available in Hadoop already at that time), but also <strong>iterative computations</strong> like graph algorithms or Machine Learning.</p>
</div>
<div class="paragraph">
<p>And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>Spark combines batch, interactive, and streaming workloads under one rich concise API.</p>
</div>
<div class="paragraph">
<p>Spark supports <strong>near real-time streaming workloads</strong> via <a href="spark-streaming/spark-streaming.adoc">Spark Streaming</a> application framework.</p>
</div>
<div class="paragraph">
<p>ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads.</p>
</div>
<div class="paragraph">
<p>Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance.</p>
</div>
<div class="paragraph">
<p>There is also support for interactive workloads using Spark shell.</p>
</div>
<div class="paragraph">
<p>You should watch the video <a href="https://youtu.be/SxAxAhn-BDU">What is Apache Spark?</a> by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop.</p>
</div>
</div>
<div class="sect2">
<h3 id="_leverages_the_best_in_distributed_batch_data_processing"><a class="anchor" href="#_leverages_the_best_in_distributed_batch_data_processing"></a>Leverages the Best in distributed batch data processing</h3>
<div class="paragraph">
<p>When you think about <strong>distributed batch data processing</strong>, <a href="varia/spark-hadoop.adoc">Hadoop</a> naturally comes to mind as a viable solution.</p>
</div>
<div class="paragraph">
<p>Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine.</p>
</div>
<div class="paragraph">
<p>For many, Spark is Hadoop++, i.e. MapReduce done in a better way.</p>
</div>
<div class="paragraph">
<p>And it should <strong>not</strong> come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all.</p>
</div>
</div>
<div class="sect2">
<h3 id="_rdd_distributed_parallel_scala_collections"><a class="anchor" href="#_rdd_distributed_parallel_scala_collections"></a>RDD - Distributed Parallel Scala Collections</h3>
<div class="paragraph">
<p>As a Scala developer, you may find Spark&#8217;s RDD API very similar (if not identical) to <a href="http://www.scala-lang.org/docu/files/collections-api/collections.html">Scala&#8217;s Collections API</a>.</p>
</div>
<div class="paragraph">
<p>It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense).</p>
</div>
<div class="paragraph">
<p>So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender.</p>
</div>
</div>
<div class="sect2">
<h3 id="_rich_standard_library"><a class="anchor" href="#_rich_standard_library"></a><a id="rich-standard-library"></a> Rich Standard Library</h3>
<div class="paragraph">
<p>Not only can you use <code>map</code> and <code>reduce</code> (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development.</p>
</div>
<div class="paragraph">
<p>It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce.</p>
</div>
</div>
<div class="sect2">
<h3 id="_unified_development_and_deployment_environment_for_all"><a class="anchor" href="#_unified_development_and_deployment_environment_for_all"></a>Unified development and deployment environment for all</h3>
<div class="paragraph">
<p>Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or <a href="spark-shell.adoc">the Spark shell</a>, or the many Spark Application Frameworks leveraging the concept of <a href="spark-rdd.adoc">RDD</a>, i.e. <a href="spark-sql.adoc">Spark SQL</a>, <a href="spark-streaming/spark-streaming.adoc">Spark Streaming</a>, <a href="spark-mllib/spark-mllib.adoc">Spark MLlib</a> and <a href="spark-graphx.adoc">Spark GraphX</a>, you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (<a href="spark-mllib/spark-mllib.adoc">Spark MLlib</a>), a structured data queries (<a href="spark-sql.adoc">Spark SQL</a>) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation.</p>
</div>
<div class="paragraph">
<p>It&#8217;s also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project.</p>
</div>
</div>
<div class="sect2">
<h3 id="_interactive_exploration_exploratory_analytics"><a class="anchor" href="#_interactive_exploration_exploratory_analytics"></a>Interactive Exploration / Exploratory Analytics</h3>
<div class="paragraph">
<p>It is also called <em>ad hoc queries</em>.</p>
</div>
<div class="paragraph">
<p>Using <a href="spark-shell.adoc">the Spark shell</a> you can execute computations to process large amount of data (<em>The Big Data</em>). It&#8217;s all interactive and very useful to explore the data before final production release.</p>
</div>
<div class="paragraph">
<p>Also, using the Spark shell you can access any <a href="spark-cluster.adoc">Spark cluster</a> as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using <code>--master</code>) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, <a href="spark-streaming/spark-streaming.adoc">Spark Streaming</a>, and Spark GraphX.</p>
</div>
<div class="paragraph">
<p>Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX).</p>
</div>
</div>
<div class="sect2">
<h3 id="_single_environment"><a class="anchor" href="#_single_environment"></a>Single Environment</h3>
<div class="paragraph">
<p>Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform.</p>
</div>
<div class="paragraph">
<p>You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or <a href="spark-streaming/spark-streaming.adoc">Spark Streaming</a> (DStreams).</p>
</div>
<div class="paragraph">
<p>Or use them all in a single application.</p>
</div>
<div class="paragraph">
<p>The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_integration_toolkit_with_rich_set_of_supported_data_sources"><a class="anchor" href="#_data_integration_toolkit_with_rich_set_of_supported_data_sources"></a>Data Integration Toolkit with Rich Set of Supported Data Sources</h3>
<div class="paragraph">
<p>Spark can read from many types of data sources&#8201;&#8212;&#8201;relational, NoSQL, file systems, etc.&#8201;&#8212;&#8201;using many types of data formats - Parquet, Avro, CSV, JSON.</p>
</div>
<div class="paragraph">
<p>Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications.</p>
</div>
</div>
<div class="sect2">
<h3 id="_tools_unavailable_then_at_your_fingertips_now"><a class="anchor" href="#_tools_unavailable_then_at_your_fingertips_now"></a>Tools unavailable then, at your fingertips now</h3>
<div class="paragraph">
<p>As much and often as it&#8217;s recommended <a href="http://c2.com/cgi/wiki?PickTheRightToolForTheJob">to pick the right tool for the job</a>, it&#8217;s not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice).</p>
</div>
<div class="paragraph">
<p>Spark embraces many concepts in a single unified development and runtime environment.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling <code>pipe()</code>).</p>
</li>
<li>
<p>DataFrames from R are available in Scala, Java, Python, R APIs.</p>
</li>
<li>
<p>Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with <a href="spark-sql-thrift-server.adoc">Thrift JDBC/ODBC Server</a> in Spark SQL).</p>
</div>
<div class="paragraph">
<p>Mind the proverb <a href="https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail">if all you have is a hammer, everything looks like a nail</a>, too.</p>
</div>
</div>
<div class="sect2">
<h3 id="_low_level_optimizations"><a class="anchor" href="#_low_level_optimizations"></a>Low-level Optimizations</h3>
<div class="paragraph">
<p>Apache Spark uses a <a href="spark-scheduler-DAGScheduler.adoc">directed acyclic graph (DAG) of computation stages</a> (aka <strong>execution DAG</strong>). It postpones any processing until really required for actions. Spark&#8217;s <strong>lazy evaluation</strong> gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more).</p>
</div>
<div class="paragraph">
<p>Mind the proverb <a href="https://en.wiktionary.org/wiki/less_is_more">less is more</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_excels_at_low_latency_iterative_workloads"><a class="anchor" href="#_excels_at_low_latency_iterative_workloads"></a>Excels at low-latency iterative workloads</h3>
<div class="paragraph">
<p>Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms.</p>
</div>
<div class="paragraph">
<p>Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives.</p>
</div>
<div class="paragraph">
<p>Spark can <a href="spark-rdd-caching.adoc">cache intermediate data in memory for faster model building and training</a>. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns.</p>
</div>
<div class="paragraph">
<p>Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory.</p>
</div>
<div class="paragraph">
<p>Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data.</p>
</div>
</div>
<div class="sect2">
<h3 id="_etl_done_easier"><a class="anchor" href="#_etl_done_easier"></a>ETL done easier</h3>
<div class="paragraph">
<p>Spark gives <strong>Extract, Transform and Load (ETL)</strong> a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem.</p>
</div>
<div class="paragraph">
<p>Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java).</p>
</div>
</div>
<div class="sect2">
<h3 id="_unified_concise_high_level_api"><a class="anchor" href="#_unified_concise_high_level_api"></a><a id="unified-api"></a> Unified Concise High-Level API</h3>
<div class="paragraph">
<p>Spark offers a <strong>unified, concise, high-level APIs</strong> for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API).</p>
</div>
<div class="paragraph">
<p>Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark).</p>
</div>
</div>
<div class="sect2">
<h3 id="_different_kinds_of_data_processing_using_unified_api"><a class="anchor" href="#_different_kinds_of_data_processing_using_unified_api"></a>Different kinds of data processing using unified API</h3>
<div class="paragraph">
<p>Spark offers three kinds of data processing using <strong>batch</strong>, <strong>interactive</strong>, and <strong>stream processing</strong> with the unified API and data structures.</p>
</div>
</div>
<div class="sect2">
<h3 id="_little_to_no_disk_use_for_better_performance"><a class="anchor" href="#_little_to_no_disk_use_for_better_performance"></a>Little to no disk use for better performance</h3>
<div class="paragraph">
<p>In the no-so-long-ago times, when the most prevalent distributed computing framework was <a href="varia/spark-hadoop.adoc">Hadoop MapReduce</a>, you could reuse a data between computation (even partial ones!) only after you&#8217;ve written it to an external storage like <a href="varia/spark-hadoop.adoc">Hadoop Distributed Filesystem (HDFS)</a>. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead.</p>
</div>
<div class="paragraph">
<p>One of the many motivations to build Spark was to have a framework that is good at data reuse.</p>
</div>
<div class="paragraph">
<p>Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn&#8217;t matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so <a href="spark-rdd.adoc">no shuffle occur</a>).</p>
</div>
<div class="paragraph">
<p>The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both.</p>
</div>
</div>
<div class="sect2">
<h3 id="_fault_tolerance_included"><a class="anchor" href="#_fault_tolerance_included"></a>Fault Tolerance included</h3>
<div class="paragraph">
<p>Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them.</p>
</div>
</div>
<div class="sect2">
<h3 id="_small_codebase_invites_contributors"><a class="anchor" href="#_small_codebase_invites_contributors"></a>Small Codebase Invites Contributors</h3>
<div class="paragraph">
<p>Spark&#8217;s design is fairly simple and the code that comes out of it is not huge comparing to the features it offers.</p>
</div>
<div class="paragraph">
<p>The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_further_reading_or_watching"><a class="anchor" href="#_further_reading_or_watching"></a><a id="i-want-more"></a> Further reading or watching</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>(video) <a href="https://youtu.be/L029ZNBG7bk">Keynote: Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks</a></p>
</li>
</ul>
</div>
</div>
</div>
</article>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../../../../../_/js/site.js"></script>
<script async src="../../../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
